{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **VR Mini Project - Image Captioning**\n\n## IMT2020039 - Anshul Jindal  \n## IMT2020535 - Shreeya Venneti\n## IMT2020094 - Riddhi Chatterjee\n## IMT2020523 - Kedar Deshpande","metadata":{}},{"cell_type":"markdown","source":"# IMPORTING LIBRARIES\n\nImporting all the relevant libraries","metadata":{}},{"cell_type":"code","source":"import cv2\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pickle\nimport string\nimport random\nfrom tqdm import tqdm","metadata":{"execution":{"iopub.status.busy":"2023-05-15T16:10:26.274695Z","iopub.execute_input":"2023-05-15T16:10:26.275568Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Uninstall the previous installed nltk library\n!pip install -U nltk\n\n# This upgraded nltkto version 3.5 in which meteor_score is there.\n!pip install nltk==3.5","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow\nfrom tensorflow import keras\nfrom keras.preprocessing import sequence\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras import Input, layers\nfrom tensorflow.keras import optimizers\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras.layers import add\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.applications.resnet50 import ResNet50\nfrom tensorflow.keras.applications import VGG16\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.layers import LSTM, Embedding, Dense, Activation, Flatten, Reshape, Dropout\nfrom tensorflow.keras.layers import Attention\nfrom nltk.translate.bleu_score import sentence_bleu\nfrom nltk.translate.meteor_score import meteor_score\nimport tensorflow as tf\nfrom keras.layers import concatenate","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Reading the Names of Images and Corresponding Captions","metadata":{}},{"cell_type":"markdown","source":"## Opening and Reading the text files","metadata":{}},{"cell_type":"code","source":"with open(\"/kaggle/input/flickr/Flickr8k/Flickr8k_text/Flickr_8k.trainImages.txt\", \"r\") as f:\n    train_img_names = f.read().split(\"\\n\")\n    train_img_names = train_img_names[:-1]\n\nwith open(\"/kaggle/input/flickr/Flickr8k/Flickr8k_text/Flickr_8k.valImages.txt\", \"r\") as f:\n    val_img_names = f.read().split(\"\\n\")\n    val_img_names = val_img_names[:-1]\n\nwith open(\"/kaggle/input/flickr/Flickr8k/Flickr8k_text/Flickr_8k.testImages.txt\", \"r\") as f:\n    test_img_names = f.read().split(\"\\n\")\n    test_img_names = test_img_names[:-1]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_img_names[0]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with open(\"/kaggle/input/flickr/Flickr8k/Flickr8k_text/Flickr8k.token.txt\", \"r\") as f:\n    captions_list = f.read().split(\"\\n\")\n    captions_list = captions_list[:-1]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"captions_list[0:6]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Thus we can see that each of the image has 5 captions associated with it. They are all independent.","metadata":{}},{"cell_type":"markdown","source":"# PREPROCESSING","metadata":{}},{"cell_type":"markdown","source":"## 1. Arranging the Captions\n\nEvery image has 5 captions associated with it (you can see above). So let us store the these all captions in a **DICTIONARY** where **Key will be the Image Name** and **Value will be the corresponding 5 Captions** for that image. Also, in this process, we can separate image names fro their captions in the above captions list. Doing this will make our further processing easier.","metadata":{}},{"cell_type":"code","source":"# Initialising the Dictionary\ncaptions_dict = {}\n\nfor i in captions_list:\n    \n    # We split on the basis of \"\\t\" token\n    # Take the first part of the split because that only rep the image name\n    img_name = i.split(\"\\t\")[0] \n    img_name = img_name[:-2]    # Removing #(Num) part from the image name\n    \n    # Second part of the split will correspond to the captions\n    img_caption = i.split(\"\\t\")[1]\n    \n    if img_name in captions_dict.keys():\n        captions_dict[img_name].append(img_caption)\n    else:\n        captions_dict[img_name] = [img_caption]  ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Print the captions for the last image\ncaptions_dict[img_name]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 2. Splitting into Train, Test and Val","metadata":{}},{"cell_type":"code","source":"# Also we split this whole dictionary into Train, Test and Val dictionaries\ntrain_dict = {}\ntest_dict = {}\nval_dict = {}\n\nfor i in train_img_names:\n    train_dict[i] = captions_dict[i].copy()\nfor i in val_img_names:\n    val_dict[i] = captions_dict[i].copy()\nfor i in test_img_names:\n    test_dict[i] = captions_dict[i].copy()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 3. Adding Sequence Tokens \n\nAdding a **startcap** token at the begining of all the TRAIN captions to mark the begining of caption and a **endcap** token at the end of the captions to mark the end of caption. We will iterate in the dictionary.","metadata":{}},{"cell_type":"code","source":"for img_name in captions_dict:\n    for i in range(len(captions_dict[img_name])):\n        if img_name in train_dict.keys():\n            captions_dict[img_name][i] = \"startcap \" + captions_dict[img_name][i] + \" endcap\"\n            train_dict[img_name][i] = \"startcap \" + train_dict[img_name][i] + \" endcap\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 4. Calculating Maximum length of Caption\n\nOut of all the captions, we will find the caption which has maximum number of words in that. That will help us in padding rest of the captions.","metadata":{}},{"cell_type":"code","source":"caption_max_length = 0\n\nfor img_name in captions_dict:\n    for i in captions_dict[img_name]:\n        if caption_max_length < len(i.split()):\n            caption_max_length = len(i.split())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"caption_max_length","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 5. Calculate Vocabulary Size\n\nWe will form a vocabulary of the words present in our captions. So lets find the **list of all the unique words** which are occuring in the captions.","metadata":{}},{"cell_type":"code","source":"vocab_list = []\n\nfor img_name in captions_dict:\n    for i in captions_dict[img_name]:\n        for w in i.split():\n            if w not in vocab_list:\n                vocab_list.append(w)\n            else:\n                continue\n                \nvocab_size = len(vocab_list)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vocab_size","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# GENERATING WORD EMBEDDINGS\n\nNow we can't use One-Hot encodings because of the obvious reasons of consuming large amount of memory. So what should be do??\n\nWell, we give our model randomly intilised 10278 vectors each with smaller dimension. The dimension can be 100, 200 anything. Our model will learn all these embeddings itself. This process is called Word Embeddings. Word Embedding matrix contain all 10278 vectors as its rows. \n\nWe will use Pre Trained Embeddings called GloVe Embeddings. The embedding dimension of GloVe is 200.","metadata":{}},{"cell_type":"markdown","source":"## Loading GloVe Embeddings","metadata":{}},{"cell_type":"code","source":"glove_embeddings = open('/kaggle/input/glove6b200d/glove.6B.200d.txt', encoding=\"utf-8\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Each line in the GloVe Embedding text file is formed of 2 things. The word itself followed by it's 200 dimensional embedding. This is all stored as a single string. We read and split this string and create vocabulary dictionary for all the words present in glove embddings. ","metadata":{}},{"cell_type":"code","source":"glove_emb_dict = {} \n\nfor embs in glove_embeddings:\n    temp = embs.split()\n    word = temp[0]\n    emb = np.asarray(temp[1:], dtype='float32')\n    \n    #Adding this into our dictionary\n    glove_emb_dict[word] = emb","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Generating Embedding Matrix\n\nNow we will create our embedding matrix. The procedure that we will follow is that we will iterate through our vocabulary list and keep on initialising an embedding for the words in the volcabulary. Whatever words are present in glove embeddings and in vocabulary, we will initialise their embeddings same as of GloVe embeddings, otherise random.","metadata":{}},{"cell_type":"code","source":"emb_dim = 200 #Same as GloVe dimension\nembedding_matrix = np.random.uniform(0, 1, (vocab_size, emb_dim))\n\n# We are iterating through vocab list.\n# We will use the position of a word in the list as index for that word for our embedding\nfor word in vocab_list:\n    if word in glove_emb_dict:\n        index_of_word = vocab_list.index(word)\n        embedding_matrix[index_of_word] = glove_emb_dict.get(word)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# MODEL DECLARATIONS","metadata":{}},{"cell_type":"markdown","source":"## 1. CNN - Model VGG16","metadata":{}},{"cell_type":"code","source":"# Loading the pretrained vgg16 model.\n\nvgg16 = VGG16()\nvgg16 = Model(inputs = vgg16.inputs, outputs = vgg16.layers[-2].output)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_features = {}\n\nfor tr_img in tqdm(train_img_names):\n    img = cv2.imread(\"/kaggle/input/flickr/Flickr8k/Flicker8k_Images\"  + \"/\" + tr_img)\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n    img = cv2.resize(img,(224,224))\n    img = np.expand_dims(img, axis=0)\n    \n    # Enabling GPU\n    with tf.device('gpu'):\n        feature_tr_img = vgg16.predict(img, verbose=0).reshape(4096)\n        \n    train_features[tr_img] = feature_tr_img","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with open(\"Train_Images_Features.pkl\", 'wb') as f:\n  pickle.dump(train_features, f)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 2. LSTM and rest of the Model\n\nNow we have extracted features of image using CNN. Now we will leverage that feature vector and the decoded sequence and keep on predicting next word in the caption sentence. Hence we need a model that takes an input image and an input word sequence, processes them, combines the resulting representations, and produces a probability distribution over the vocabulary for each word in the output sequence.","metadata":{}},{"cell_type":"code","source":"# feature input -> first path \nin1 = Input(shape = (1, 4096))\n\nfeat_l1 = Dropout(0.5)(in1)\nfeat_l2 = Dense(emb_dim, activation = 'relu')(feat_l1)\n\n# sequence input -> second path\nin2 = Input(shape=(caption_max_length,))\nemb = Embedding(vocab_size, emb_dim, weights=[embedding_matrix], trainable=False, mask_zero=False)(in2)\n#emb = Dense(emb_dim, activation = 'relu')(emb)\n\ncomb_l1 = add([feat_l2, emb])\n\nseq_l1 = Dropout(0.1)(comb_l1)\nseq_l2 = LSTM(emb_dim, return_sequences = True)(seq_l1)\n\nseq_l3 = Dropout(0.1)(seq_l2)\nseq_l4 = LSTM(emb_dim, return_sequences = True)(seq_l3)\n\nseq_l5 = Dropout(0.1)(seq_l4)\nseq_l6 = LSTM(emb_dim)(seq_l5)\n\n\ncomb_l2 = add([Reshape((emb_dim, ))(feat_l2), seq_l6])\ncomb_l3 = Dense(emb_dim, activation = 'relu')(comb_l2)\n\n# output\noutput = Dense(vocab_size, activation = 'softmax')(comb_l3)\n\n# compile model\nmodel = Model(inputs = [in1, in2], outputs = output)\nmodel.compile(loss = 'categorical_crossentropy', \n             optimizer = Adam(amsgrad = True, learning_rate = 0.0005))\n\n# to prevent overfitting\ncp = EarlyStopping(patience = 3, restore_best_weights= True)\n\nprint(model.summary())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# BUILDING DATALOADER\n\nWe will call our fuction **Data Generator** to generate batch_size number of training samples at a time. But before that let us read our training image features that we stored in a pickle file.","metadata":{}},{"cell_type":"code","source":"# train_features = pickle.load(open(\"/kaggle/input/d/riddhich/train-features/Train_Images_Features.pkl\", 'rb'))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def data_generator(features_dict, captions_dict, batch_size):\n    \n    # Now we have input as image features and caption.\n    # Output id the next word in the caption\n    # Let us declare lists to store them all for a batch\n    img_features = []\n    input_cap = []\n    output_cap = []\n    \n    # To count the number of images processed\n    itr = 0\n    \n    while True:\n        \n        for img_name, cap_list in captions_dict.items():\n            \n            # Get the relevant image features\n            img_feat = features_dict[img_name].reshape(1, 4096)\n\n            for caption in cap_list:\n                \n                # Encode the caption\n                caption_seq = [vocab_list.index(word) for word in caption.split(\" \") if word in vocab_list]\n\n                for i in range(1, len(caption_seq)):\n                    \n                    in_caption = caption_seq[:i]\n                    out_caption = caption_seq[i]\n                    \n                    # Pad the input sequences\n                    in_caption = pad_sequences([in_caption], maxlen=caption_max_length)[0]\n                    \n                    # Convert the output value to one hot \n                    out_caption = to_categorical([out_caption], num_classes=vocab_size)[0]     \n                    \n                    img_features.append(img_feat)\n                    input_cap.append(in_caption)\n                    output_cap.append(out_caption)\n                    \n                itr += 1\n                    \n                if itr == batch_size:\n                    yield ([np.array(img_features), np.array(input_cap)], np.array(output_cap))\n                        \n                    itr = 0\n                    img_features = []\n                    input_cap = []\n                    output_cap = []\n            ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# TRAINING CNN-LSTM MODEL","metadata":{}},{"cell_type":"code","source":"number_of_epochs = 50\nbatch_size = 160\nsteps = (len(train_dict)*5)//batch_size\n\ngen = data_generator(train_features, train_dict, batch_size)\n\nwith tf.device('gpu'):\n    model.fit(gen, epochs=number_of_epochs, steps_per_epoch=steps, verbose=1, callbacks=[cp])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with open('Trained_Model.pkl', 'wb') as f:\n  pickle.dump(model, f)\n\nmodel.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# TESTING THE MODEL\n\nWe tested the model on various random test images. We defined a **get_caption function** that takes the image and calls our model repetitively to predict the whole caption for that image. Some of the outputs that we get are shown below.","metadata":{}},{"cell_type":"code","source":"# model = pickle.load(open(\"/kaggle/input/generateddata/Trained_Model.pkl\", 'rb'))\n# print(model.summary())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_caption(image):\n    image = image.reshape(1, 1, 4096)\n    \n    in_caption = 'startcap'\n    pred_word = \"\"\n    length_predicted = 0\n    \n    while(pred_word != 'endcap' and length_predicted <= caption_max_length) :\n        caption_seq = [vocab_list.index(word) for word in in_caption.split() if word in vocab_list]\n        caption_seq = pad_sequences([caption_seq], maxlen=caption_max_length)\n        \n        # Calling our model\n        with tf.device('gpu'):\n            pred_softmax = model.predict([image,caption_seq], verbose=0)\n        pred_index = np.argmax(pred_softmax)\n        pred_word = vocab_list[pred_index]\n        \n        in_caption += ' ' + pred_word\n        length_predicted += 1\n        \n    caption = in_caption.split()\n    # Remove 'startcap' and 'endcap' from the predicted caption\n    caption = caption[1:-1]\n    caption = ' '.join(caption)\n    return caption","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Pick a test image that you want to caption.\ntest_image_names = []\n\ntest_image_numbers = [1,11,21,87,91]\nfor num in test_image_numbers:\n    test_image_names.append(test_img_names[num])\n\nfor imgs in test_image_names:\n    # Reading the image and processing it\n    img = cv2.imread(\"/kaggle/input/flickr/Flickr8k/Flicker8k_Images\"  + \"/\" + imgs)   \n    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n    img = cv2.resize(img,(224,224))\n    img = np.expand_dims(img, axis=0)\n\n    # Calling the Model\n    pred = vgg16.predict(img, verbose=0).reshape(1, 4096)\n\n    # Displaying the image\n    x = plt.imread(\"/kaggle/input/flickr/Flickr8k/Flicker8k_Images\"  + \"/\" + imgs)\n    plt.imshow(x)\n    plt.show()\n\n    # printing the caption\n    caption = get_caption(pred)\n    print(caption)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# EVALUATION USING BLEU SCORE","metadata":{}},{"cell_type":"code","source":"scores_list = []\ntest_images_list = test_img_names.copy()\n#test_images_list = random.sample(test_images_list, 100)\n\nfor img_name in tqdm(test_images_list):   \n    predictions_list = []\n    img = cv2.imread(\"/kaggle/input/flickr/Flickr8k/Flicker8k_Images\"  + \"/\" + img_name)\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n    img = cv2.resize(img,(224,224))\n    img = np.expand_dims(img, axis=0)\n\n    with tf.device('gpu'):\n        pred = vgg16.predict(img, verbose=0).reshape(1, 4096)\n\n    pred = get_caption(pred)\n    predictions_list.append(pred)\n\n    reference = test_dict[img_name].copy()\n\n    pred_words = pred.split()\n    score = sentence_bleu(reference, pred)\n    scores_list.append(score)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def mode(arr):\n    vals,counts = np.unique(arr, return_counts=True)\n    mode = np.argmax(counts)\n    return vals[mode]\n\nprint(\"Mean BLEU: \" + str(np.mean(scores_list)))\nprint(\"Max BLEU: \" + str(np.max(scores_list)))\nprint(\"Mode BLEU: \" + str(mode(scores_list)))\nprint(\"Median BLEU: \" + str(np.median(scores_list)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# EVALUATION USING METEOR SCORE","metadata":{}},{"cell_type":"code","source":"scores_list = []\ntest_images_list = test_img_names.copy()\n#test_images_list = random.sample(test_images_list, 100)\n\nfor img_name in tqdm(test_images_list):   \n    predictions_list = []\n    img = cv2.imread(\"/kaggle/input/flickr/Flickr8k/Flicker8k_Images\"  + \"/\" + img_name)\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n    img = cv2.resize(img,(224,224))\n    img = np.expand_dims(img, axis=0)\n\n    with tf.device('gpu'):\n        pred = vgg16.predict(img, verbose=0).reshape(1, 4096)\n\n    pred = get_caption(pred)\n    predictions_list.append(pred)\n\n    reference = test_dict[img_name].copy()\n\n    pred_words = pred.split()\n    score = meteor_score(reference, pred)\n    scores_list.append(score)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def mode(arr):\n    vals,counts = np.unique(arr, return_counts=True)\n    mode = np.argmax(counts)\n    return vals[mode]\n\nprint(\"Mean METEOR: \" + str(np.mean(scores_list)))\nprint(\"Max METEOR: \" + str(np.max(scores_list)))\nprint(\"Mode METEOR: \" + str(mode(scores_list)))\nprint(\"Median METEOR: \" + str(np.median(scores_list)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}